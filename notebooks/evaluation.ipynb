{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different types of metrics with pytorch implementations, metrics chosen from [1](https://arxiv.org/pdf/1806.07755.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically all of these can operate in the feature space of a pre-trained model on the ImageNet dataset. Map input into any semantically meaningful feature space; imagenet models tend to have a good feature representation, so using them is safe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Score, most widely adopted in literature [source](https://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation: https://github.com/sbarratt/inception-score-pytorch\n",
    "\n",
    "Note, [this paper](https://arxiv.org/pdf/1801.01973.pdf) warns against using the inception score.\n",
    "\n",
    "Inception score simply evaluates the distribution of the (generated) images.\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Inception_score):\n",
    "The Inception Score is maximized when the following conditions are true:\n",
    "1. The entropy of the distribution of labels predicted by the Inceptionv3 model for the generated images is minimized. In other words, the classification model confidently predicts a single label for each image. Intuitively, this corresponds to the desideratum of generated images being \"sharp\" or \"distinct\".\n",
    "2. The predictions of the classification model are evenly distributed across all possible labels. This corresponds to the desideratum that the output of the generative model is \"diverse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fréchet Inception Distance (FID), [source](https://arxiv.org/abs/1706.08500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implementation: https://github.com/mseitzer/pytorch-fid\n",
    "\n",
    "FID compares the distribution of generated images with the distribution of a set of real images (\"ground truth\").\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance), FID is the current standard metric for assessing the quality of generative models as of 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/27 [00:03<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/pytorch_fid/__main__.py\", line 3, in <module>\n",
      "    pytorch_fid.fid_score.main()\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/pytorch_fid/fid_score.py\", line 313, in main\n",
      "    fid_value = calculate_fid_given_paths(args.path,\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/pytorch_fid/fid_score.py\", line 259, in calculate_fid_given_paths\n",
      "    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size,\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/pytorch_fid/fid_score.py\", line 243, in compute_statistics_of_path\n",
      "    m, s = calculate_activation_statistics(files, model, batch_size,\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/pytorch_fid/fid_score.py\", line 228, in calculate_activation_statistics\n",
      "    act = get_activations(files, model, batch_size, dims, device, num_workers)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/pytorch_fid/fid_score.py\", line 132, in get_activations\n",
      "    for batch in tqdm(dataloader):\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/tqdm/std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/_utils.py\", line 644, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"/home/nashir/miniconda3/envs/cap5516-final/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n",
      "    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
      "RuntimeError: Trying to resize storage that is not resizable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m pytorch_fid --num-workers 2 data/chest_xray/train/NORMAL data/chest_xray/test/NORMAL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to modify to have data folders where all images have same size. Test on fashion mnist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.97it/s]\n",
      "FID:  87.44324079827993\n"
     ]
    }
   ],
   "source": [
    "!python -m pytorch_fid --num-workers 2 data/fmnist/set1-real data/fmnist/set2-real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.32it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.23it/s]\n",
      "FID:  -6.110252797952853e-05\n"
     ]
    }
   ],
   "source": [
    "!python -m pytorch_fid --num-workers 2 data/fmnist/set1-real data/fmnist/set1-real"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Inception Distance (KID), [source](https://arxiv.org/pdf/1801.01401.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation: https://github.com/abdulfatir/gan-metrics-pytorch/blob/master/kid_score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode score, [source](https://arxiv.org/abs/1612.02136)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implementation: https://github.com/xuqiantong/GAN-Metrics/blob/45dc74fac8b2452d5f37f035bba7352f873c7f91/metric.py#L356-L361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in maximum mean discrepancies (MMDs), [source](https://arxiv.org/abs/1511.04581)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Implementation: https://github.com/ZongxianLee/MMD_Loss.Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier two-sample test, also called 1-NN classifier [source](https://arxiv.org/abs/1610.06545)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add this later if time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: human evaluation tends to be biased towards the visual quality of generated samples and neglect the overall distributional characteristics, which are important for unsupervised learning, [source](https://arxiv.org/pdf/1806.07755.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap5516-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
